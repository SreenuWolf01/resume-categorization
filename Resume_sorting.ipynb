{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "##################################################\n",
        "### Importing necessary libraries ################\n",
        "##################################################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/resume_sorting/gpt_dataset.csv'\n",
        "\n",
        "# Download NLTK data (required for tokenization, stop words, and lemmatization)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbCodhut1ry_",
        "outputId": "de4ba814-fc55-4af3-96e8-c9955c835c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz5UxMCByZY6",
        "outputId": "47d0db9e-07de-49b5-97a4-398de489c8a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Category                                             Resume\n",
            "0  Frontend Developer  As a seasoned Frontend Developer, I have a pro...\n",
            "1   Backend Developer  With a solid background in Backend Development...\n",
            "2    Python Developer  As a Python Developer, I leverage my expertise...\n",
            "3      Data Scientist  With a background in Data Science, I possess a...\n",
            "4  Frontend Developer  Experienced Frontend Developer with a passion ...\n"
          ]
        }
      ],
      "source": [
        "#########################\n",
        "#### Load the dataset ###\n",
        "#########################\n",
        "df = pd.read_csv(file_path, encoding='utf-8')\n",
        "\n",
        "# Display the first few rows to verify the data structure\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################\n",
        "### Performing text preprocessing #########\n",
        "###########################################\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize lemmatizer and stop words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean and preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    # Join back into string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to the resume column\n",
        "# Assuming the resume text is in 'Resume' column, adjust if necessary\n",
        "df['cleaned_resume'] = df['Resume'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "vbeONyLHNvtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################\n",
        "### Using TF-IDF for words vectorization ##########\n",
        "###################################################\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "\n",
        "# Fit and transform the cleaned resumes\n",
        "X = tfidf_vectorizer.fit_transform(df['cleaned_resume'])\n",
        "\n",
        "# The target variable (categories) remains the same\n",
        "y = df['Category']\n",
        "\n",
        "# Optionally, print the shape of the resulting matrix to verify\n",
        "print(\"Shape of TF-IDF matrix:\", X.shape)"
      ],
      "metadata": {
        "id": "0mL_dtS-M-_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45fefc83-fb09-49ca-db02-745ecdff5397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of TF-IDF matrix: (400, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################\n",
        "### Splitting the data and training the model ####\n",
        "##################################################\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle = True)\n",
        "#random_state ensures the reproducabilty which means, the same split is used for\n",
        "#the future usage of the model\n",
        "#X_train: The subset of the input features (X) used to train the model.\n",
        "#X_test: The subset of the input features (X) used to evaluate the model's performance.\n",
        "#y_train: The subset of the target labels (y) corresponding to X_train.\n",
        "#y_test: The subset of the target labels (y) corresponding to X_test.\n",
        "\n",
        "# Encode labels for XGBoost\n",
        "#covert text categories into numbers\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "print(\"Logistic Regression model trained.\")\n",
        "\n",
        "# Initialize and train Random Forest model\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "print(\"Random Forest model trained.\")\n",
        "\n",
        "# Initialize and train XGBoost model\n",
        "xgb_clf = XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss')\n",
        "xgb_clf.fit(X_train, y_train_encoded)\n",
        "print(\"XGBoost model trained.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9N8UwbTGUQG",
        "outputId": "ebee45aa-3f8a-4c23-f597-ca6213e653ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model trained.\n",
            "Random Forest model trained.\n",
            "XGBoost model trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "y_pred_xgb = label_encoder.inverse_transform(xgb_clf.predict(X_test))\n",
        "\n",
        "# Calculate and print accuracy\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "\n",
        "# Print detailed classification reports\n",
        "print(\"\\nLogistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_log_reg))\n",
        "\n",
        "print(\"\\nRandom Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "print(\"\\nXGBoost Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SusKUmj1Kpmu",
        "outputId": "e14c94a5-1e10-4422-fb81-2d7390f4c0d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 1.0\n",
            "Random Forest Accuracy: 1.0\n",
            "XGBoost Accuracy: 1.0\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "                                    precision    recall  f1-score   support\n",
            "\n",
            "                 Backend Developer       1.00      1.00      1.00        10\n",
            "                    Cloud Engineer       1.00      1.00      1.00        17\n",
            "                    Data Scientist       1.00      1.00      1.00        12\n",
            "                Frontend Developer       1.00      1.00      1.00         9\n",
            "              Full Stack Developer       1.00      1.00      1.00         7\n",
            "         Machine Learning Engineer       1.00      1.00      1.00         7\n",
            "Mobile App Developer (iOS/Android)       1.00      1.00      1.00        11\n",
            "                  Python Developer       1.00      1.00      1.00         7\n",
            "\n",
            "                          accuracy                           1.00        80\n",
            "                         macro avg       1.00      1.00      1.00        80\n",
            "                      weighted avg       1.00      1.00      1.00        80\n",
            "\n",
            "\n",
            "Random Forest Classification Report:\n",
            "                                    precision    recall  f1-score   support\n",
            "\n",
            "                 Backend Developer       1.00      1.00      1.00        10\n",
            "                    Cloud Engineer       1.00      1.00      1.00        17\n",
            "                    Data Scientist       1.00      1.00      1.00        12\n",
            "                Frontend Developer       1.00      1.00      1.00         9\n",
            "              Full Stack Developer       1.00      1.00      1.00         7\n",
            "         Machine Learning Engineer       1.00      1.00      1.00         7\n",
            "Mobile App Developer (iOS/Android)       1.00      1.00      1.00        11\n",
            "                  Python Developer       1.00      1.00      1.00         7\n",
            "\n",
            "                          accuracy                           1.00        80\n",
            "                         macro avg       1.00      1.00      1.00        80\n",
            "                      weighted avg       1.00      1.00      1.00        80\n",
            "\n",
            "\n",
            "XGBoost Classification Report:\n",
            "                                    precision    recall  f1-score   support\n",
            "\n",
            "                 Backend Developer       1.00      1.00      1.00        10\n",
            "                    Cloud Engineer       1.00      1.00      1.00        17\n",
            "                    Data Scientist       1.00      1.00      1.00        12\n",
            "                Frontend Developer       1.00      1.00      1.00         9\n",
            "              Full Stack Developer       1.00      1.00      1.00         7\n",
            "         Machine Learning Engineer       1.00      1.00      1.00         7\n",
            "Mobile App Developer (iOS/Android)       1.00      1.00      1.00        11\n",
            "                  Python Developer       1.00      1.00      1.00         7\n",
            "\n",
            "                          accuracy                           1.00        80\n",
            "                         macro avg       1.00      1.00      1.00        80\n",
            "                      weighted avg       1.00      1.00      1.00        80\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Select the best model (Logistic Regression chosen for simplicity)\n",
        "best_model = log_reg\n",
        "\n",
        "# Save the model and TF-IDF vectorizer\n",
        "joblib.dump(best_model, 'resume_classifier_model.pkl')\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "print(\"Model and vectorizer saved.\")\n",
        "\n",
        "# Function to classify a new resume\n",
        "def classify_resume(resume_text):\n",
        "    # Preprocess the resume\n",
        "    cleaned_resume = preprocess_text(resume_text)\n",
        "    # Transform using the trained TF-IDF vectorizer\n",
        "    resume_tfidf = tfidf_vectorizer.transform([cleaned_resume])\n",
        "    # Predict the category\n",
        "    prediction = best_model.predict(resume_tfidf)\n",
        "    return prediction[0]\n",
        "\n",
        "# Test with a sample resume\n",
        "sample_resume = \"\"\"\n",
        "Skilled in Python, machine learning, and data analysis. Experienced in building predictive models using scikit-learn and TensorFlow. Proficient in SQL and big data tools like Hadoop.\n",
        "\"\"\"\n",
        "predicted_category = classify_resume(sample_resume)\n",
        "print(\"Predicted job category for sample resume:\", predicted_category)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmP_cWSXYOkK",
        "outputId": "6af948b0-3ba3-4897-a761-71f78cecc222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and vectorizer saved.\n",
            "Predicted job category for sample resume: Data Scientist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample resume text for Data Engineer\n",
        "cloud_engineer = \"\"\"\n",
        "John Doe has over 4 years of experience designing, implementing, and maintaining cloud infrastructure across AWS, Azure, and Google Cloud. He is skilled in Terraform, AWS CloudFormation, Docker, Kubernetes, Jenkins, and GitLab CI/CD, with strong programming skills in Python, Bash, and Go. John specializes in building scalable, secure cloud environments, setting up CI/CD pipelines, managing Kubernetes clusters (EKS), and implementing cloud security best practices like IAM policies and VPC configurations. He has successfully led cloud migration projects, automated deployments, and developed monitoring solutions using Prometheus, Grafana, and the ELK Stack. John holds a Bachelor’s degree in Computer Science from the University of Washington and is certified as an AWS Solutions Architect, Azure Administrator, and Kubernetes Administrator.\n",
        "\"\"\"\n",
        "\n",
        "# Classify the sample resume\n",
        "predicted_category1 = classify_resume(cloud_engineer)\n",
        "print(\"Predicted job category for Data Engineer resume:\", predicted_category1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuyXAAcMZpQX",
        "outputId": "ea48125d-7f9c-4c2d-ba5c-b1e29e1167ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted job category for Data Engineer resume: Cloud Engineer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python_dev = \"\"\"\n",
        "John Doe has over 4 years of experience building and maintaining software applications using Python. He is skilled in frameworks like Django, Flask, and FastAPI, and has hands-on experience developing RESTful APIs, backend systems, and automation scripts. John is also proficient in working with SQL and NoSQL databases such as PostgreSQL, MySQL, and MongoDB. He has a strong background in writing efficient, clean, and scalable code, along with experience in deploying applications using Docker and AWS services. Additionally, he is familiar with Agile methodologies, Git version control, and testing tools like PyTest and Unittest. John holds a Bachelor's degree in Computer Science from the University of Washington and is passionate about developing high-performance software solutions that solve real-world problems.\n",
        "\"\"\"\n",
        "# Classify the sample resume\n",
        "predicted_category2 = classify_resume(python_dev)\n",
        "print(\"Predicted job category for Data Engineer resume:\", predicted_category2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZQIr0EQZt73",
        "outputId": "609ef5ce-441b-45a2-a086-cb8797e203ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted job category for Data Engineer resume: Python Developer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Along with resume classification I want to classify the resume and rank the id or name of the candidates who are suitable for a role with the help of 5 top skills of the person in that role, and filter the top 10 persons in that category, or take input from the user to filter ‘x’ number of people for a certain role and sort them for futher interview or screening."
      ],
      "metadata": {
        "id": "exyLaCcKkwZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Download the resumes_sample.zip file\n",
        "!wget https://github.com/florex/resume_corpus/raw/master/resume_samples.zip\n",
        "\n",
        "# Step 2: Unzip the downloaded file\n",
        "!unzip resume_samples.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr5rSllogPyA",
        "outputId": "0de3ca44-40ce-438b-ef26-ab1e8e93c8fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-26 05:31:45--  https://github.com/florex/resume_corpus/raw/master/resume_samples.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/florex/resume_corpus/master/resume_samples.zip [following]\n",
            "--2025-04-26 05:31:46--  https://raw.githubusercontent.com/florex/resume_corpus/master/resume_samples.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63818300 (61M) [application/zip]\n",
            "Saving to: ‘resume_samples.zip’\n",
            "\n",
            "resume_samples.zip  100%[===================>]  60.86M   185MB/s    in 0.3s    \n",
            "\n",
            "2025-04-26 05:31:48 (185 MB/s) - ‘resume_samples.zip’ saved [63818300/63818300]\n",
            "\n",
            "Archive:  resume_samples.zip\n",
            "  inflating: resume_samples.txt      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 3: Read the extracted file\n",
        "data = []\n",
        "with open('resume_samples.txt', 'r', encoding='utf-8', errors='replace') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split(':::')\n",
        "        if len(parts) == 3:\n",
        "            id_, occupations, resume_text = parts\n",
        "            data.append((id_, occupations, resume_text))\n",
        "\n",
        "# Step 4: Create a DataFrame\n",
        "new_df = pd.DataFrame(data, columns=['ID', 'Occupations', 'resume_text'])\n",
        "\n",
        "# Step 5: View the DataFrame\n",
        "new_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "collapsed": true,
        "id": "g63E8Op_lNHi",
        "outputId": "e8a0a7ee-0303-46a4-ec40-984d57130a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Name  \\\n",
              "0  C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\1.h...   \n",
              "1  C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\10....   \n",
              "2  C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\100...   \n",
              "3  C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\100...   \n",
              "4  C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\100...   \n",
              "\n",
              "                                         Occupations  \\\n",
              "0  Database Administrator;Database Administrator;...   \n",
              "1  Database Administrator;SQL, Microsoft PowerPoi...   \n",
              "2  Oracle Database Administrator;Oracle Database ...   \n",
              "3  Amazon Redshift Administrator and ETL Develope...   \n",
              "4  Scrum Master;Oracle Database Administrator/ Sc...   \n",
              "\n",
              "                                              Resume  \n",
              "0  Database Administrator <span class=\"hl\">Databa...  \n",
              "1  Database Administrator <span class=\"hl\">Databa...  \n",
              "2  Oracle Database Administrator Oracle <span cla...  \n",
              "3  Amazon Redshift Administrator and ETL Develope...  \n",
              "4  Scrum Master Scrum Master Scrum Master Richmon...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67f6cb78-ca7b-4c57-9b47-fc1df7c52799\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Occupations</th>\n",
              "      <th>Resume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\1.h...</td>\n",
              "      <td>Database Administrator;Database Administrator;...</td>\n",
              "      <td>Database Administrator &lt;span class=\"hl\"&gt;Databa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\10....</td>\n",
              "      <td>Database Administrator;SQL, Microsoft PowerPoi...</td>\n",
              "      <td>Database Administrator &lt;span class=\"hl\"&gt;Databa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\100...</td>\n",
              "      <td>Oracle Database Administrator;Oracle Database ...</td>\n",
              "      <td>Oracle Database Administrator Oracle &lt;span cla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\100...</td>\n",
              "      <td>Amazon Redshift Administrator and ETL Develope...</td>\n",
              "      <td>Amazon Redshift Administrator and ETL Develope...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\100...</td>\n",
              "      <td>Scrum Master;Oracle Database Administrator/ Sc...</td>\n",
              "      <td>Scrum Master Scrum Master Scrum Master Richmon...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67f6cb78-ca7b-4c57-9b47-fc1df7c52799')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-67f6cb78-ca7b-4c57-9b47-fc1df7c52799 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-67f6cb78-ca7b-4c57-9b47-fc1df7c52799');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-52c76cb1-885d-4f53-87b4-523b2793bb73\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-52c76cb1-885d-4f53-87b4-523b2793bb73')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-52c76cb1-885d-4f53-87b4-523b2793bb73 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "new_df",
              "summary": "{\n  \"name\": \"new_df\",\n  \"rows\": 29780,\n  \"fields\": [\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29780,\n        \"samples\": [\n          \"C:\\\\Workspace\\\\java\\\\scrape_indeed\\\\java_dev_part_0\\\\2149.html#13605\",\n          \"C:\\\\Workspace\\\\java\\\\scrape_indeed\\\\web_dev_part_1\\\\2513.html#29695\",\n          \"C:\\\\Workspace\\\\java\\\\scrape_indeed\\\\it_sa_part_1\\\\2856.html#11347\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Occupations\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 27743,\n        \"samples\": [\n          \"Android Developer Trainee;Python Developer Intern;Cable Technician;Engineer I Intern;Java (2 years);C++ (2 years);Verilog (1 year);Python (1 year);Matlab (2 years);Android;Embedded\",\n          \"SENIOR SOFTWARE ENGINEER;RAILS DEVELOPER;RAILS DEVELOPER;SOFTWARE ENGINEER (RUBY ON RAILS);Android (4 years);iOS (4 years);JavaScript (7 years);RAILS (4 years);Ruby (9 years);PHP;MYSQL;Git;CSS\",\n          \"Sr. Java Developer;Java Developer;Java Developer;Java Developer;Eclipse;Ejb;J2ee;Java;Hibernate;Spring;Jboss;Jms;jquery;Jsf;Jsp;Rmi;Servlets;Struts;Application server;C++;Html;Javascript;Json;Perl\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Resume\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29780,\n        \"samples\": [\n          \"Senior Pega Developer Senior Pega <span class=\\\"hl\\\">Developer</span> Senior Pega Developer - Bank Of America Plano, TX ? Over 10 years of IT experience including 6 years of PEGA/PRPC experience in Design and development of Business Process Management (BPM) using Pega Rules Process Commander (PRPC).\\ufffd ? 3.5 years of Java/J2EE experience in developing Enterprise Client server applications.\\ufffd ? As a PEGA Senior Developer, expertise in designing and implementing Enterprise class structure for the organization based on Pega best practices and guardrails, which can be reusable and extendable.\\ufffd ? Hands on Experience with PEGA 7.1.9\\ufffd ? Experience with Agile/Scrum Development methodology\\ufffd ? Hands on experience in PRPC flow design, activities, integration connectors (connect SOAP, connect SQL), services and screen design (harness, section, flow action), Agents & SLAs.\\ufffd ? Experience in using Decision and Declarative(Declare Expression, Declare Onchange, Declare Trigger and Declare Index) rules.\\ufffd ? Hands on experience with PRPC Reporting features (List View, Summary View and Report Definitions, Charts).\\ufffd ? Good hand on experience in Gathering the Requirements for the enhancements from the business, Analyzing and Providing Estimates.\\ufffd ? Good Experience in PRPC V6.x features and Hands on experience on PRPC construction and debugging tools such as Tracer, PLA,Preflight Clipboard, DB Trace and System management Application (SMA).\\ufffd ? Experience in implementing the Business Intelligence Exchange (BIX)\\ufffd ? Experience in conducting Design and Code reviews.\\ufffd ? Knowledge in Performance tuning of the applications.\\ufffd ? Experience in PRPC integration in Enterprise Environment using Service-SOAP, Connect-REST, File Listener, Connect-JMS, Connect-SOAP & Connect-SQL\\ufffd ? Experience in Bulk Creation of Cases using CSV file upload, File Service.\\ufffd ? Experience in Creating Parse XML, Parse Stream and Parse Delimitted rules.\\ufffd ? Ability to troubleshoot and resolve complex production issues.\\ufffd ? Strong communication and Presentation skills. Sponsorship required to work in the US Work Experience Senior Pega Developer Bank Of America - Plano, TX September 2015 to Present The various assignments with which I have been associated are as follows (in chronological order):\\ufffd Project 1: LRM DocGen (Letter Request Management)\\ufffd Client: Bank Of America, Plano, TX,USA\\ufffd Employer: Tata Consultancy Services Sept 2015 - Till Date\\ufffd Role: Senior Pega Developer\\ufffd Environment: PRPC 7.1, Oracle 10g, XML Spy, SOAPUI.\\ufffd Project Description:\\ufffd LRM (Letter Request Management) Document Generation project's main goal is to develop a configurable letter (Trigger/Package/Template) and automate the process of sending the letters to Bank Of America customers and Stake holders. Different letters includes Appraisal Letters, Incomplete Information notices, Acknowledgements, Solicitation letters, Statements etc.\\ufffd LRM integrates with multiple fulfilment service providers to mail the letter to customers, attorneys and interested parties with relevant details prefilled in letter.\\ufffd \\ufffd Responsibilities:\\ufffd \\ufffd Worked with Line of Business and End users to understand the Use Cases and requirements.\\ufffd \\ufffd Involved in story creation, story grooming, Estimation, Coding, Testing and Code migration to higher environments.\\ufffd \\ufffd Attending daily stand up & Issue resolution meetings.\\ufffd \\ufffd Created High Level Class structure, Workflows and Rule sets in Pega PRPC.\\ufffd \\ufffd Designed the Letter Request flow and Integration service to accept the Case creation request from different systems.\\ufffd \\ufffd Created the Standard and Advanced agents for Background processing.\\ufffd \\ufffd Created different portal for various users like Admin, Setup user, Approver user.\\ufffd \\ufffd Customized the OOTB Work Transfer functionality and User Interface.\\ufffd \\ufffd Created different PRPC components like Flows, Activities, Decision tables, Decision Maps, Declarative expressions, Constraints and Indexes etc.\\ufffd \\ufffd Worked on Developing UI components like Section, Layouts, Harness(New, Perform, Confirm and Review Harness).\\ufffd \\ufffd Created Data models, Activities, Access groups and Access Roles.\\ufffd \\ufffd Worked on integrating PRPC with External Applications using Connect SOAP, Connect -REST, Connect-JMS and Connect SQL\\ufffd \\ufffd Worked on implementing Assignment SLAs.\\ufffd \\ufffd Coordinated with offshore team and played a key role in understanding the requirements\\ufffd \\ufffd Taking care of Business Ad Hoc Requests to update the Production Data\\ufffd \\ufffd Efficiently handled core production fixes Senior Pega Developer Bank Of America - Plano, TX October 2014 to August 2015 Role: Senior Pega Developer\\ufffd Environment: PRPC 7.1, Oracle 10g, XML Spy, SOAP UI\\ufffd Project Description:\\ufffd DLN (Document List and Notification) designed to notify systems and users of Document ingestion progress based on document activity such as successful document ingestion & Delivery exceptions etc. Notifications implemented via different channels like Web service notification, MQ Notification & Email Notification.\\ufffd \\ufffd Responsibilities\\ufffd \\ufffd Worked with Line of Business and End users to understanding the Use Cases and requirements.\\ufffd \\ufffd Involved in story creation, story grooming, and Estimation, Coding, Testing and Code migration to higher environments.\\ufffd \\ufffd Identify all the interfaces required for the application & POC study before the construction phase.\\ufffd \\ufffd Created standard and advanced agents for processing the Event Notifications as a background process.\\ufffd \\ufffd Developed the User interface for viewing the transaction history of notifications.\\ufffd \\ufffd Involved in analyzing & fixing the Scrum and UAT issues.\\ufffd \\ufffd Created Connect-SOAP and Connect-JMS integration.\\ufffd \\ufffd Created different PRPC components like, Activities, Decision tables, Decision Maps, Declarative expressions, Constraints and Indexes etc. PRPC Developer Hanover Insurance, MA January 2013 to March 2014 Environment: PRPC 6.3, Oracle 9, XML Spy, SOAP UI\\ufffd \\ufffd Project Description:\\ufffd Real time Rating(RTR) is ACORD based interface to accept requests to quote both the Homeowners and Personal Auto Lines of Business. If the Agent likes the returned quote, phase I of the project will have them bridge the quote into our POS application, utilizing our existing Business Partners application Pre-fill and Flood. Having flooded the quote into our POS application, the Agent will be navigated as deep into the POS page flow as the bridged data allows where they will be prompted to complete the entry process and submit the quote for issue. To achieve this end, both new and existing functionality will be built, re-used and/or modified and extended utilizing a combination of new state of the art and existing technologies.\\ufffd \\ufffd Responsibilities\\ufffd \\ufffd Worked with Business Analysts to understand the requirements.\\ufffd \\ufffd Created a Design and Technical Specification documents.\\ufffd \\ufffd Designed the Real Time Rating workflow for Auto and Home Quotes.\\ufffd \\ufffd Created Service SOAP to create the Auto and Home Quotes.\\ufffd \\ufffd Create the integration with MainFrame system to get the Premium.\\ufffd \\ufffd Worked with Release Management team to deploy the componenets.\\ufffd \\ufffd Single Point of contact from onsite for Test and UAT Team. PRPC Developer Hanover Insurance, MA April 2012 to December 2012 Role: PRPC Developer\\ufffd Environment: PRPC 6.3, Oracle 9, XML Spy, SOAP UI,SVN.\\ufffd \\ufffd Project Description:\\ufffd Account Credit Score is used by the underwriter for making decisions towards writing the bonds for the accounts that Hanover has. This will help the bond department to write the good business under Commercial Line of business.\\ufffd The scope of this project is:\\ufffd 1. Generate the score matrix for statement period for each financial statement defined at an account level.\\ufffd 2. Define the standard spectrum and allow modifications to the spectrum. Note: Modifications to spectrum will be allowed only by administrator role. Normal user cannot modify the spectrum and weights.\\ufffd 3. Colour coding and Equity check on the UI screen based on certain requirements will be part of this phase of implementation and will be handled.\\ufffd 4. Design a PEGA batch process to process daily bonds transactions.\\ufffd 5. Design Financial scoring UI for underwriters.\\ufffd \\ufffd Responsibilities\\ufffd \\ufffd Studying and understanding the functional specifications and writing technical specifications.\\ufffd \\ufffd Implemented Bonds agent for batch processing\\ufffd \\ufffd Created the technical specification and unit test plans.\\ufffd \\ufffd Writing new activities, decision rules.\\ufffd \\ufffd Involved in bug fixing.\\ufffd \\ufffd Communicate & interact with Business analyst and Quality analyst for clarifications, issues. Java & PRPC Developer Hanover Insurance, MA May 2010 to March 2012 Project 5: Quote Proposal for Personal & Commercial lines\\ufffd Client: Hanover Insurance, MA,USA May 2010 - Mar 2012\\ufffd Employer: NTT Data Global Delivery Services ltd.\\ufffd Role: Java & PRPC Developer\\ufffd Environment: Java/J2EE,PRPC 6.3, Oracle 9, XML Spy, SOAP UI\\ufffd \\ufffd Project Description:\\ufffd One of the major themes within the PL Strategy is the ease of doing business. In our quest to increase our book of business, one of the key areas we are focusing on within this initiative is to make our systems attractive as well as easy to use. Today agents have the choice of placing their business with whichever company they please, and one of the decision points involved in this is the ease and flexibility that a company's application provides to them. Quote proposal provides the estimated premium and Cross selling information.\\ufffd \\ufffd Responsibilities:\\ufffd \\ufffd Involved in Tech Design and Technical specification documents and defining standards using NTT Data's Proprietary Architectural Practice methodology.\\ufffd \\ufffd Involved in discussions with Data architect for preparing Field requirement specifications.\\ufffd \\ufffd Involved in requirements gathering, guided business analyst to understand the existing Quote detail.\\ufffd \\ufffd Using top down approach created data objects, implemented JDBC connectivity.\\ufffd \\ufffd Created client project to interact with Process server deployed Bpel process and WebSphere deployed loss handling service for PL QPP.\\ufffd \\ufffd Involved in Pega PL and CL pay plan services and Marketing message service class hierarchy design and developing web services.\\ufffd \\ufffd Involved in XSD objects creation.\\ufffd \\ufffd Worked on activities, maps, decisions rules. Java & PRPC Developer Hanover Insurance, MA February 2009 to April 2010 Role: Java & PRPC Developer\\ufffd Environment: Java/J2EE,PRPC 6.3, Oracle 9, XML Spy, SOAP UI\\ufffd \\ufffd Project Description:\\ufffd Insurance Score System application developed to get the Credit report of the insured. Credit score is the main driven element while underwriting process. Choice Point is a provider of insurance and credit score information. Currently Choice Point is single source of information costing us millions of dollars each year for Hanover. The first step towards this vision with a focus on alternatives to Choice Point's Insurance Scoring Model is to begin collecting the raw Credit Report data (Phase I). This will allow Hanover at some point in the future to create a proprietary Credit Scoring Model with equal or superior predictive properties, reduced expense, and an aligned credit score that can be used across all personal lines of business (single insurance score for auto, home, umbrella, etc). This Phase will be further divided into three releases.\\ufffd Responsibilities\\ufffd \\ufffd Participated in BRD, Tech Design and Technical specification documents walk through.\\ufffd \\ufffd Received KT and Involved in bug fixes of release 1.\\ufffd \\ufffd Involved in writing new business process flow, activities, maps, decision tables.\\ufffd \\ufffd Created rules to connect to the external data base and retrieve the data. Java Developer Hanover Insurance, MA November 2006 to January 2009 Environment: Java, SOAP, RAD v6.0, Windows XP, IBM DB2.\\ufffd \\ufffd Project Description:\\ufffd Provide quote, underwrite and issuance capabilities for an account. The agent/HUB user will have the ability to quote and issue multiple lines of business for an account. An account Quote Proposal Package will be provided. Account level data will prefill to the lines of business eliminating redundant data entry. The user will have the option to rate one, multiple or all quotes for an account. All referral messages will be displayed in Account View in addition to data from multiple locations within the individual line of business quote flows. Each quote will be issued separately through the current line of business issuance flow. Issuance will be also allowed from the Account View one line of business at a time. This project will provide the ability to determine account eligibility in an upfront triage process.\\ufffd Responsibilities\\ufffd \\ufffd Studying and understanding the functional specifications and Technical specifications.\\ufffd \\ufffd Involved in development and unit testing of Web services by using RAD.\\ufffd \\ufffd Generating Caster objects using the XSD's provided with Rational Application developer.\\ufffd \\ufffd Implemented JDBC connectivity to interact with IBM DB2 data base from Java application according to the Hanover enterprise standards.\\ufffd \\ufffd Unit testing of web service using IBM process server provided test client.\\ufffd \\ufffd Developing the modules and interfaces for application's health check. Java Developer Zenith Consultancy - IN June 2006 to September 2006 Environment: Java, SOAP, RAD v6.0, Windows XP, IBM DB2.\\ufffd \\ufffd Project Description:\\ufffd ZCS project deals with the Resume Repository software with a centralized database. The Application will allow creation of Job Profiles for different categories of jobs mentioned and as changed from time to time. It will facilitate the creation of clients and the orders placed by the clients and as well as a module to help track the order status. ZCS application consists of both Web application and Desktop application. So that it can help the job seekers to post the resumes through the web application and consultancy people to search the resumes based on Desktop application It also consists of a smart search facility, which will help in searching and sorting the resumes according to recruiter's requirement.\\ufffd Responsibilities\\ufffd \\ufffd Developing Servlets, JSP as server side programming for generating HTML Pages\\ufffd \\ufffd Data Base Access / Storage using JDBC Using Tomcat web server for production\\ufffd \\ufffd Developed user interaction html forms. Education Bachelor of Technology in Computer Science Engineering Jawaharlal Nehru University (JNTU) - Hyderabad, ANDHRA PRADESH, IN 2004 Skills J2EE (3 years), Java (5 years), JavaScript (Less than 1 year), WebSphere (1 year), XML (7 years) Additional Information Technical Skills:\\ufffd \\ufffd Languages Java, J2EE\\ufffd Web Technologies HTML, JavaScript, XML, XSL\\ufffd BPM Tools PRPC 7.X, 6.X, PRPC 5.X,\\ufffd Application Servers Websphere, Weblogic, Jboss server\",\n          \"Senior ColdFusion Application Developer Senior ColdFusion Application <span class=\\\"hl\\\">Developer</span> Senior ColdFusion Application Developer - Sedgwick Claim Services Inc New Port Richey, FL Work Experience Senior ColdFusion Application Developer Sedgwick Claim Services Inc October 2018 to Present \\ufffd Programmed the Two-Way Text application allowing hundreds of clients in the US and Canada to receive and reply to text messages sent to their employees through the web application.\\ufffd \\ufffd Provide application support and programming for the company's flagship applications, ADA Link and Leave Link, which allows thousands of nationwide employees to file Disability, ADA and FMLA claims online. Freelance Software Developer Coast Guard February 2018 to Present ColdFusion/Full Stack Application Development\\ufffd \\ufffd Design, develop, program and maintain FloatPlanWizard.com. A mobile web application serving thousands of users that electronically prepares, delivers, and monitors a US Coast Guard certified Float Plan. Project Lead, Guitar Center Syntel Inc June 2009 to February 2018 \\ufffd Charged to provide project leadership and mentoring for offshore Web Development team.\\ufffd \\ufffd Manage 40+ enterprise web sites, perform advanced ColdFusion programming, server administration, business analysis, application design/development, and project management.\\ufffd \\ufffd Managed/Lead development for used gear intake application, which processes an average of 2,000 street buys daily, resulting in upward of $2M in sales weekly.\\ufffd \\ufffd Administer 25 Private Label websites that increased sales and improved customer service and satisfaction.\\ufffd \\ufffd Lead development of CMS for private label sites allowing site owners to manage all content from one centralized location.\\ufffd \\ufffd Generate several thousand sales weekly through programming for the sale of Platinum items and additional sales through clearance item applications, providing national visibility to local listings.\\ufffd \\ufffd Increase significantly the visibility and bookings through Spearhead development of corporate and local versions of Rehearsal Studio.\\ufffd \\ufffd Lead the development of CMS for Guitar Studio sites, providing site administrators with streamlined, automated, and centralized processes.\\ufffd \\ufffd Create and program multiple business reports; streamlining and automating the prior paper process, resulting in increased productivity and accuracy. Internet Systems Administrator / Senior ColdFusion Developer Guitar Center Inc - Westlake Village, CA June 2003 to June 2009 Westlake Village, CA 6/03 - 6/09\\ufffd Country's largest musical instruments retailer\\ufffd Internet Systems Administrator / Senior ColdFusion Developer\\ufffd \\ufffd Provided ColdFusion programming, ColdFusion Administration, Windows 2003 Server administration, Load Balancing, site maintenance, database management, site-architecture, and project leadership for the company's 20+ high traffic e-commerce, enterprise, and private label websites.\\ufffd \\ufffd Generated more revenue than the firm's 220 retail locations through transforming guitarcenter.com from brochure ware to e-commerce.\\ufffd \\ufffd Spearheaded web portal development; automated department tasks and distributed company information/reports electronically, saving 100's of man-hours per week and decreasing payroll expenses.\\ufffd \\ufffd Programmed Facility Management Application, streamlining repairs and maintenance administration.\\ufffd \\ufffd Created comprehensive sales analysis and payout online reports, centralizing to one location.\\ufffd \\ufffd Designed and programmed E-Commerce application, providing the charitable department a resource for generating funds through the sale of musical memorabilia.\\ufffd \\ufffd Created the Instrument Rental Application, generating thousands of online rentals per year. musicarts.com\\ufffd \\ufffd Programmed web application for the sale of used musical instruments and gear, generating millions of dollars in sales annually.\\ufffd \\ufffd Supported Human Resources, programming the Training Certification Tracking application, utilized by 8,000 sales associates and the employment pre-assessment test, decreasing the hiring process time. Web Developer / Multimedia Producer Wald Media Productions - Culver City, CA 2001 to 2003 Culver City, CA 2001-2003\\ufffd Firm provided web development and multimedia services to corporate and entertainment companies.\\ufffd Web Developer / Multimedia Producer\\ufffd \\ufffd Launched multimedia company, managing all aspects of business administration including operations including operations, finance and budgeting, business development, and client relations.\\ufffd \\ufffd Developed international e-commerce capabilities for a major Central American music producer, resulting in a 25% increase in music sales. Web Developer Americana Financial - New York, NY 2001 to 2002 Provided design, programming, and project leadership for corporate and company web sites. Education Liberal Arts Rockland Community College Diploma in Television Broadcasting Center for the Media Arts Skills Ajax, Coldfusion, Css, Web services, Devops, Cms, Api, Javascript, Bootstrap, Json, Oop, Svn, Xml, Sdlc, Mysql, Sql, Html5, Soap, Rest, jquery, HTML, Sql Server Links http://www.linkedin.com/in/larry-wald-coldfusion http://GuitarCenter.com Additional Information KEY TECHNICAL SKILLS\\ufffd ColdFusion, SOAP, REST, SQL, JavaScript, MYSQL, XML, SDLC, AJAX, JSON, OOP, Web Services, API Development, jQuery, HTML5, Bootstrap, CSS, JIRA, MURA CMS, DevOps, SVN\\ufffd \\ufffd ADDITIONAL SKILLS\\ufffd Site Administration, Program Development, Project Management, Leadership, Offshore Management, Customer Service, Increasing Revenues, Process Improvement, Automation, Troubleshooting\\ufffd \\ufffd Sample URL's: GuitarCenter.com FloatPlanWizard.com ADALink.com LeaveLink.com\",\n          \"Information Security Project Manager Information <span class=\\\"hl\\\">Security</span> Project Manager Project Manager - MBA, PMP, CSM w/ Public Trust (High) Washington, DC SUMMARY OF QUALIFICATIONS\\ufffd \\ufffd \\ufffd Ability to present technology strategy to all levels of company management.\\ufffd \\ufffd Proven ability providing executive support, business strategy and program management.\\ufffd \\ufffd Knowledgeable of cybersecurity best practices, capabilities, frameworks and standards.\\ufffd \\ufffd Exceptional knowledge of government enterprise IT operations and procedures.\\ufffd \\ufffd Strong financial analysis, public policy, project and strategic management skills Authorized to work in the US for any employer Work Experience Information Security Project Manager Various State, Local and Federal Government Agencies - Washington, DC June 2019 to Present Develops and updates security-related documentation to reflect the security posture of the government IT system, as directed by the government agency managers.\\ufffdAssists government customers in reviewing or modifying security documentation for quality and accuracy of processes.\\ufffdMonitors and controls changes to information systems and assesses the security impact of those changes.\\ufffdSupports government leads on enterprise security support, risk and vulnerability management (CAPs and POA&M) and audit finding remediation.\\ufffdAssists government managers with internal Cyber Awareness, Training and Outreach efforts.\\ufffdManages project efforts to mature cybersecurity operations for government agencies.\\ufffd \\ufffd \\ufffd Assists the CISO with development of security operations, threat monitoring, vulnerability management, intrusion detection/prevention, data loss prevention, incident response, etc.\\ufffd \\ufffd Assists the CISO on efforts to enhance security operations through system security plans updates, policy/procedure development, organizational change management and alignment to network infrastructure \\ufffd \\ufffd Develops, updates, and reviews security authorization and accreditation documentation (A&A) to ensure consistency with NIST 800-53 and FISMA laws, NIST Risk Management and Cybersecurity Frameworks (800-SPs), CISA and OMB Directives.\\ufffd \\ufffd Manages cybersecurity project portfolio on efforts related to Trusted Internet Connections, Network Access Control, SOC Implementation, Continuous Diagnostic Monitoring (CDM), Risk Profiles (Current and Target), and High Value Assets, etc.\\ufffd Senior PMO Project Manager Citizant - Executive Office Immigration Review (EOIR) - Falls Church, VA July 2018 to April 2019 Executive Office Immigration Review (EOIR).\\ufffdDepartment of Justice (DOJ) Office of Information Technology.\\ufffdDevelops project related documents, project plans, and requirements; develops technical documents, such as Statements of Works, Standard Operating Procedures and recommends and implements changes to improve daily operations and procedures; develops strategies to recommend new projects and documented tasks; leverages current agency IT tools and resources to enhance operations and collaboration; lead efforts across division to increase collaboration across federal government agency.\\ufffdProvides project management and business analyst support for optimal decision making and strategic alignment of agency objectives.\\ufffdActivities include managing schedule, gathering application, staging, testing and requirements for federal facilities and coordination of third-party vendors activities.\\ufffd \\ufffd \\ufffd Manages infrastructure projects that enhance the IT operational posture of the U.S. Immigration Courts, consisting of over 60 locations nationally and hundreds of end users; manages the Digital Audio Recording (DAR) Upgrade Project, an initiative leading to the replacement of 400 courtroom audio/voice recording systems; manages the Electronic Digital Docket System (EDDS) Project that deploys 300 digital signage units to federal courthouses. Project Manager NOW Technologies (NTS) - Largo, MD May 2017 to February 2018 Prince George's County Government. Office of Information Technology (PG-OIT), Enterprise Architecture Division. Present Network Infrastructure Services Technical Project Manager Develops project related documents, project plans, and requirements; develops technical documents, such as Statements of Works, Standard Operating Procedures and Site Build Sheets; recommends and implements changes to improve daily operations and procedures; develop strategies to recommend new projects and documented tasks; leverages current agency IT tools and resources to enhance operations and collaboration; leads efforts across division to increase collaboration across Enterprise Architecture with the creation of Network Appliance User Groups; motivates, encourages and recommends cross-training of agency staff on network appliances that have multi-use business cases. Provides project management and business analyst support to Network Appliance Staging and Enhancements, Infrastructure Technology Support, and Municipal Fiber Projects.\\ufffd \\ufffd \\ufffd Manages staging projects that enhance the IT operational posture of the county government. Implements Network Access Control (ForeScout CounterACT) in Phases II and III across 18,000 network endpoints; revises agencies information security policies (Acceptable Use and Access Control Standards) to align with county Administrative Procedure 119; recommends further enhancement projects for Splunk (log mining and aggregation), Riverbed (Net Flow traffic), SolarWinds (network monitoring), etc.\\ufffd \\ufffd Manages projects network infrastructure installations and upgrade projects, such as the FY18 Technology Refresh for core network components. Sites include Inglewood III, Kent Police HQ, RMS Building, among other county facilities.\\ufffd \\ufffd \\ufffd Manages the FY18 PC Refresh Project (Phase I), an initiative leading to the replacement of 2,000 laptop and desktop computers across all county government agencies. Activities include developing project chart, schedule and other key artifacts that lead to successful implementation and completion.\\ufffd \\ufffd \\ufffd Manages fiber build projects that expand the footprint of the Inter-County Broadband Network (ICBN); coordinates activities with the INET and consortium members, such as the University of Maryland University College. IT Project Manager/Business Analyst Federal Working Group (FWG) - Washington, DC March 2016 to March 2017 Provides project management and program analytical support to Office of the Comptroller of the Currency, Deputy Comptroller for Infrastructure and Operations (DCIO), Management and Controls (M&C) and Telecommunications, Messaging and Operations (TMO) directorates. Possess strong understanding of federal government IT governance, culture and organization. Strong experience working in MS Project Server and SharePoint environments for large IT organizations.\\ufffd \\ufffd Portfolio Management:\\ufffd \\ufffd Supports DCIO Program Management Office (PMO) through the entire project life cycle; including developing charters, plans, schedules, resource estimates, cost plans/budgets, ; staffing plans, change and release management efforts, encourages support for director policies; and manage project and portfolio documentation on SharePoint and MS Project sites.\\ufffd \\ufffd \\ufffd Identifies and leads efforts to improve business processes, business outputs and timelines; develops and manages capacity planning efforts (resource allocation estimates versus actuals) for directorate; provides analytical data (using various internal tools) to support on-going trend, root cause analysis, performance tracking, and quantitative project metrics.\\ufffd \\ufffd Technical Project Management:\\ufffd \\ufffd Provides subject matter expertise, with regards to unified communications, to aide in the shaping of projects and business requirements for mobility (Citrix/VPN), wireless/mobile device management (MDM), unified communications (Cisco/Avaya/Skype), WAN-LAN/VoIP, Video/VTC delivery. Requires interaction with upper-level IT management, leads, architects, and engineers to ensure processes are efficient, sustainable, repeatable, and predictable. Understands and documents business requirements as requested.\\ufffd \\ufffd \\ufffd Provides project management support to directorate for ongoing level of effort activities and milestone projects. Liaises with both internal stakeholders and customers, and external channel partners and suppliers to discuss and create business and technical requirements. Project Manager (PS Wireless Broadband) ENGILITY - Washington, DC November 2014 to August 2015 Support the Broadband Technologies Opportunities Program (BTOP) for the state of New Jersey (JerseyNet - Early Builder Project) in developing an interoperable public safety wireless broadband (4G/LTE/WiFi/DAS/Microwave) network dedicated for first responders. Acquires strong understanding of Systems Engineering Lifecycle for wireless telecommunications network deployments: Planning, Design Review, Sourcing; Staging and Production, Solutions and Implementation, Testing, Training, Operations and Maintenance and Transition\\ufffd \\ufffd Program Management (Acting Program Manager):\\ufffd \\ufffd Participates in contract modification process for funding requisitions and purchase orders (POs); participates in recruiting process for Radio Frequency (RF) engineering support.\\ufffd \\ufffd \\ufffd Serves as liaison between finance and contracting departments and corresponding client representatives to ensure smooth project operations.\\ufffd \\ufffd \\ufffd Acquires strong knowledge of business and operational processes, including contracting, subcontracting, vendor management (negotiations, signing, implementation, payments).\\ufffd \\ufffd \\ufffd Acquires in depth background of budgetary development, burden rate for subsequent, contracting vehicles, financials analysis and solvency, and compliance and ROI determination\\ufffd \\ufffd Project Management Organization:\\ufffd \\ufffd Participates in development of Project Management work plans and key milestone deliverables for JerseyNet: Communications Plan, Quality Plan, Risk Management Plan, Site Location Plan, Technical Documentation Plan, Change Order Process and Management Plan, System/Network Design Plan, Implementation and Testing Plan, Transition Plan, Disaster Recovery and Special Operations Plan, Training Plan; further revises plans as a key deliverable and project closeout\\ufffd \\ufffd \\ufffd Maintains and manages Integrated Master Schedule (IMS) for JerseyNet for Project Executive, Team Leads and designated State of New Jersey stakeholders;\\ufffd \\ufffd \\ufffd Communicates across project leads/project partners to acquire strong understanding of business processes/ work flows for project; Project Manager (Broadband Technology) Office of the Chief Technology Officer (OCTO) - Washington, DC December 2011 to September 2014 Digital Inclusion Initiative (Connect.DC) Broadband Coordinator (Project Manager) Provides project management consulting, support and assistance to District agencies and directs District-wide technology service initiatives within assigned focus area of digital inclusion. Plans, organizes, analyzes, evaluates, and controls projects assigned to incumbent through application of project management principles, practices, and techniques. Coordinates the District's initiative to promote digital inclusion across the city and lead the effort by advocating strategies to expand access to technology in traditionally underserved communities within the District of Columbia.\\ufffd \\ufffd \\ufffd Develops business case for digital inclusion centers plan, an inter-agency partnership to expand broadband usage at city's community and recreational centers with refurbished government computers and laptops.\\ufffd \\ufffd \\ufffd Conducts test case/quality assurance to support development of crowd-sourcing content management application for community-based organizations and agency partners.\\ufffd \\ufffd \\ufffd Serves as liaison to the Initiative's Community Advisory Board; assists Director with board development and leadership activities: organizational research, development of by-laws, vetting and selection of members, setting board agenda and writing meeting minutes, etc.\\ufffd \\ufffd \\ufffd Assist in the development of the Digital Inclusion Strategy, Sub-Grant Program, Mobile Technology Lab Deployment, Community Outreach and Partnership Strategies. Project Management Consultant Hillcrest Management Group - Washington, DC June 2008 to November 2011 Developed firm strategy and identify opportunities that provide consulting solutions. Provide strategic sales support for contracting requirements. Conducted strategic marketing (due diligence, market research and requirement analysis) of client offer within 30-day to 6-month sales cycles. Created, led and managed project\\ufffd management efforts from the Interest Phase through and Post Proposal phases (craddle to grave) for federal government agencies.\\ufffd \\ufffd Project Management. Acquired strong understanding of project management concepts, principles and best practices for telecommunications and technology sectors.\\ufffd \\ufffd \\ufffd Managed Project Plans across all Planning Groups (Initiating, Planning, Executing, Monitoring & Controlling and Closing)\\ufffd \\ufffd \\ufffd Managed Project Plans across Knowledge Areas (Scope, Time, Cost, Quality, Human Resource, Communications, Risk Management and Procurement\\ufffd \\ufffd \\ufffd Managed implementation, milestones, budget, scope and stakeholder communication matters \\ufffd \\ufffd Broadband Strategy. Reviewed grant applications for the Broadband Technology Opportunities Program (BTOP), a $4.7 billion federal grant program established as part of the American Recovery and Reinvestment Act (ARRA). Examined project benefit, technical feasibility and business strategy of grant proposals against program requirements to spur economic growth through broadband deployment. Grant proposals total nearly $10 million in funding authority. Assessed cost/benefit (risk analysis) and made technology strategy recommendations. Served as subject matter expert on fixed-mobile convergence matters. Project Manager (Business Strategy) Motorola International - Berlin April 2006 to August 2006 Conducted market research, policy analysis as well as examined business case for WiMax deployment in South Africa. Supported Go-to-Market efforts across the Networks and Enterprise division in Europe, Middle East and Africa (EMEA) region. Analyzed Electronic Communications Convergence legislation and related policy matters to assess impact on South African subsidiary operations. Developed competitive landscape (ecosystem) for South African wireless broadband marketplace. Examined third-party funding sources: regional development banks, corporate venture capital, European Union structural funds, among others for network build out. Served as subject matter expert on fixed-mobile convergence matters. Telecommunications Policy Specialist Robert Bosch Foundation Fellowship - Bonn, OH June 2003 to August 2004 T-Mobile International, Public & Regulatory Affairs\\ufffd Regulatory Authority for Telecommunications & Post (RegTP/Bundesnetzagentur)\\ufffd \\ufffd Conducted research on wireless policy issues impacting trans-Atlantic telecom markets, such as spectrum policy, digital broadband television, Voice over Internet Protocol (VoIP), health effects of mobile communications (EMR), consumer billing and regulatory review of major telecom mergers. Attended spectrum planning meeting to optimize company's technology-product mix offer. Established company's first global public policy Best Practice on EMR. Provided regular reports on American telecommunications sector developments. Congressional Affairs Manager CTIA - The Wireless Association - Washington, DC June 2000 to May 2003 Developed strategic planning of public policy agenda of wireless industry before Congress and the Federal Communications Commission (FCC). Researched and analyzed federal legislation impacting commercial wireless industry. Developed decision papers and briefing materials for Capitol Hill meetings, conferences, workshops, and conventions. Education Master's in Business Administration (MBA) University of South Carolina - Columbia, SC Bachelor of Arts in Political Science & Economics Tulane University - New Orleans, LA Skills MS Access, MS Excel, MS Project, MS Word, MS Word, SharePoint, Deltek, GovWin, SQL Server, Tableau, ESRI (ArcGIS), Excel, Powerpoint, Program Management, PMP, German (Advanced-C2) Certifications/Licenses Certified Scrum Master Certificate in Project Management (CPM) Project Management Professional (PMP) Chief Data Officer (CDO) - Level 1 Groups ESRI Telecommunications Users International Institute of Business Analysis Microsoft Project User Group (MPUG) Technology Business Management (TBM) Council Project Management Institute (PMI) Scrum Alliance Additional Information Clearances: Public Trust Clearance (High)\\ufffd CCNA Cyber Ops (pending)\\ufffd \\ufffd \\ufffdAbility to present technology strategy to all levels of company management \\ufffd \\ufffd \\ufffdExperience within a carrier, network supplier, industry trade group and government IT provider \\ufffd \\ufffd \\ufffdProven ability providing executive support, business strategy and government relations\\ufffd \\ufffd \\ufffdStrong financial analysis, public policy, project and strategic management skills\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EI1bnuWmo0bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "# Load saved model and vectorizer\n",
        "model = joblib.load('resume_classifier_model.pkl')\n",
        "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "\n",
        "# Load new dataset (assumed to have 'Name' and 'Resume' columns)\n",
        "#new_df = pd.read_csv('new_resume_dataset.csv')  # Update path as needed\n",
        "\n",
        "# Preprocess resumes\n",
        "new_df['cleaned_resume'] = new_df['Resume'].apply(preprocess_text)\n",
        "\n",
        "# Transform resumes to TF-IDF vectors\n",
        "X_new = vectorizer.transform(new_df['cleaned_resume'])\n",
        "\n",
        "# Predict job roles\n",
        "new_df['Predicted_Role'] = model.predict(X_new)\n",
        "\n",
        "# Count applicants per role\n",
        "role_counts = new_df['Predicted_Role'].value_counts()\n",
        "print(\"\\nNumber of Applicants by Job Role:\")\n",
        "for role, count in role_counts.items():\n",
        "    print(f\"{role}: {count}\")\n",
        "\n",
        "# Define key skills for each role (based on your dataset)\n",
        "key_skills = {\n",
        "    'Data Scientist': ['python', 'machine learning', 'statistics', 'sql', 'tensorflow'],\n",
        "    'Cloud Engineer': ['aws', 'azure', 'docker', 'kubernetes', 'cloud'],\n",
        "    'Backend Developer': ['java', 'node.js', 'rest api', 'mongodb', 'spring'],\n",
        "    'Frontend Developer': ['javascript', 'react', 'css', 'html', 'typescript'],\n",
        "    'Full Stack Developer': ['javascript', 'python', 'django', 'react', 'sql'],\n",
        "    'Machine Learning Engineer': ['python', 'deep learning', 'pytorch', 'tensorflow', 'sklearn'],\n",
        "    'Mobile App Developer (iOS/Android)': ['swift', 'kotlin', 'flutter', 'android', 'ios'],\n",
        "    'Python Developer': ['python', 'django', 'flask', 'pandas', 'numpy']\n",
        "    # Add 'Data Engineer' if in dataset, else map to similar role\n",
        "}\n",
        "\n",
        "# Rank candidates by skills for each role\n",
        "top_candidates = defaultdict(list)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "for role in key_skills:\n",
        "    # Filter candidates for the role\n",
        "    role_df = new_df[new_df['Predicted_Role'] == role]\n",
        "    if role_df.empty:\n",
        "        print(f\"\\nNo applicants for {role}\")\n",
        "        continue\n",
        "\n",
        "    # Get indices of key skills in TF-IDF matrix\n",
        "    skill_indices = [np.where(feature_names == skill)[0][0] for skill in key_skills[role] if skill in feature_names]\n",
        "\n",
        "    # Calculate skill scores (sum of TF-IDF values for key skills)\n",
        "    role_X = vectorizer.transform(role_df['cleaned_resume'])\n",
        "    skill_scores = np.sum(role_X[:, skill_indices].toarray(), axis=1)\n",
        "\n",
        "    # Rank candidates\n",
        "    ranked_indices = np.argsort(skill_scores)[::-1][:10]  # Top 10\n",
        "    for idx in ranked_indices:\n",
        "        name = role_df.iloc[idx]['Name']\n",
        "        score = skill_scores[idx]\n",
        "        top_candidates[role].append((name, score))\n",
        "\n",
        "    # Print top 10\n",
        "    print(f\"\\nTop 10 Candidates for {role}:\")\n",
        "    for name, score in top_candidates[role]:\n",
        "        print(f\"{name}: Skill Score = {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "tUcqW0mnmUXI",
        "outputId": "433c0c74-425a-4f4e-a923-2abf35106bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-c92dac5fb72d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Preprocess resumes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_resume'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Resume'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Transform resumes to TF-IDF vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-dea59cb63d41>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Remove stop words and lemmatize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Join back into string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-dea59cb63d41>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Remove stop words and lemmatize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Join back into string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mshortest\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_exceptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmorphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   2104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m             \u001b[0;31m# 1. Apply rules once to the input to get y1, y2, y3, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2106\u001b[0;31m             \u001b[0mforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2108\u001b[0m         \u001b[0;31m# 2. Return all that are in the database (and check the original too)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mapply_rules\u001b[0;34m(forms)\u001b[0m\n\u001b[1;32m   2080\u001b[0m         \u001b[0msubstitutions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMORPHOLOGICAL_SUBSTITUTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2082\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2083\u001b[0m             return [\n\u001b[1;32m   2084\u001b[0m                 \u001b[0mform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}